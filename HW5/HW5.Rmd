---
title: "{Raquel Buezo Tordesillas} {A20410771}, {Pablo Medrano}
  {A20410758}"
output:
  html_document: default
  pdf_document: default
  word_document: default
---

```{r}
library(ISLR)


Auto=na.omit(Auto)
dim(Auto)

Auto$origin=as.factor(Auto$origin)
myData<-subset(Auto,select = -name)  

```


1. BEST SUBSET SELECTION 

```{r}

# install.packages("leaps")
library(leaps)

regfit.full=regsubsets(myData$mpg~.,myData)

reg.summary=summary(regfit.full)

reg.summary$adjr2
which.max (reg.summary$adjr2)

```
a)So the best adjusted R2 is 0.8183822 for the model with 6 variables.


b)The coefficient estimates associated with this 6 variable model are:
```{r}
coef(regfit.full ,6)
```

c)The adjusted R2 versus the number of variables is:
```{r}
plot(reg.summary$adjr2,xlab="Number of Variables",ylab="Adjusted R2")

```


2. FORWARD SELECTION

```{r}

regfit.fwd=regsubsets(myData$mpg~.,data=myData,method="forward")
reg.summary.fwd=summary(regfit.fwd)

reg.summary.fwd$adjr2
which.max (reg.summary.fwd$adjr2)

```
a)So the best adjusted R2 is 0.8183822 for the model with 6 variables.


b)The coefficient estimates associated with this 6 variable model are:
```{r}
coef(regfit.fwd ,6)
```


c)The adjusted R2 versus the number of variables is:
```{r}
plot(reg.summary.fwd$adjr2,xlab="Number of Variables",ylab="Adjusted R2")

```

d)As we can see from the adjusted R2, the best result is the same for best subset and forward selection. 


e)No, it picks the same subsets of k features for the same k as we can see below. 
```{r}
reg.summary
reg.summary.fwd

```



3. BACKWARD SELECTION

```{r}

regfit.bkwd=regsubsets(myData$mpg~.,data=myData,method="backward")
reg.summary.bkwd=summary(regfit.bkwd)

reg.summary.bkwd$adjr2
which.max (reg.summary.bkwd$adjr2)

```
a)So the best adjusted R2 is 0.8183822 for the model with 6 variables.


b)The coefficient estimates associated with this 6 variable model are:
```{r}
coef(regfit.bkwd ,6)
```


c)The adjusted R2 versus the number of variables is:
```{r}
plot(reg.summary.bkwd$adjr2,xlab="Number of Variables",ylab="Adjusted R2")

```

d)As we can see from the adjusted R2, the best result is the same for best subset and backward selection. 


e)No, it picks the same subsets of k features for the same k as we can see below.
```{r}
reg.summary
reg.summary.bkwd

```



VALIDATION AND CROSS-VALIDATION


4. validation set approach

Using the single test set approach on the models generated by best-subset, which subset had the lowest validation error.
```{r}
set.seed(1)

train=sample(c(TRUE,FALSE),nrow(myData),rep=TRUE)
val=(!train)


myData.train=myData[train,]
myData.val=myData[val,]

View(myData.train)  
View(myData.val)   


```


a. What is the best validation error?
```{r}

#Best subset selection on training data
regfit.trainbest=regsubsets(myData.train$mpg~.,data=myData.train)

#Create model matrix for validation set data
val.mat=model.matrix(myData.val$mpg~.,data=myData.val)

#Compute validation set errors for best model of each size
val.errors=rep(NA,7)
for (i in 1:7){
  coefi=coef(regfit.trainbest,id=i)

  pred=val.mat[,names(coefi)]%*%coefi

  val.errors[i]=mean((myData.val$mpg-pred)^2)
}

val.errors
which.min(val.errors)

```


b. Plot the validation error as a function of k, the number of features.
```{r}
plot(val.errors,xlab="Number of features",ylab="Validation Erorr")

```


c. Show the coefficients.
```{r}
coef(regfit.trainbest,which.min(val.errors))
```


d. Is this result different than the one you generated in question 1 for best subsets?

Yes, the coefficients are different and in this case the subset chosen was the one with 7 features instead of 6.


e. Now that you figured out the features to use, retrain your model on the full data set. Show the coefficients you yield.
```{r}
regfit.testfull=regsubsets(myData$mpg~.,data=myData)
coef(regfit.testfull,7)

```
We can see that the best 7 feature model on the full data set has a different set of variables than the best 7 feature model on the training data set. 



5. K-fold cross validation

Using 3-fold cross validation.

```{r}

k=3
set.seed(1)

#create a vector that allocates each observation to one of the folds
folds=sample(1:k, nrow(myData),replace=TRUE)                          #by default sample assigns equal probability to each group

cv.errors=matrix(NA,k,7,dimnames=list(NULL,paste(1:7)))

```

a. What is the best validation error?
```{r}

#j is the hold that is going to be the validation set for that case
for(j in 1:k){
  best.kfit=regsubsets(myData[folds!=j,]$mpg~.,data=myData[folds!=j,])   #best subset fit on training data (the other 2 folds)
  
  val.mat2=model.matrix(myData[folds==j,]$mpg~.,data=myData[folds==j,])  #model matrix for validation set data
  
  for(i in 1:7){
    coefi=coef(best.kfit,id=i)
    predb=val.mat2[,names(coefi)]%*%coefi

    cv.errors[j,i]=mean((myData$mpg[folds==j]-predb)^2)
  }
}

mean.cv.errors=apply(cv.errors,2,mean)      #apply mean to the columns of the matrix 
mean.cv.errors
which.min(mean.cv.errors)

```


b. Plot the validation error as a function of k, the number of features.
```{r}
plot(mean.cv.errors,xlab="Number of features",ylab="Validation Erorr")

```


c. Show the coefficients. We have different coefficients for each training set and since k=3, we will have 3 possible training sets. 
```{r}
for(j in 1:k){
  best.kfit=regsubsets(myData[folds!=j,]$mpg~.,data=myData[folds!=j,])
  View(coef(best.kfit,7))
}

```


d. Is this result different than the one you generated in question 1 for best subsets?

Yes, they are all different than the coefficients obtained with best subset selection.


e. Now that you figured out the features to use, retrain your model on the full data set. Show the coefficients you yield.
```{r}
regfit.ktestbest=regsubsets(myData$mpg~.,data=myData)
coef(regfit.ktestbest,7)

```
Obviously the same one as the one obtained using the validation set approach since they are both using 7 features.